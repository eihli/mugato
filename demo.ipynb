{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2b4b6-102b-4578-9360-4c1600ad9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import pygame\n",
    "import pygame.freetype\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639aef4-2008-492b-af67-abf0248867f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.utils import create_combined_dataloader\n",
    "from mugato.mugato import MugatoConfig, Mugato, TransformerConfig\n",
    "from mugato.nano_gpt import GPTConfig, GPT, Block, LayerNorm\n",
    "from mugato.utils import data_home, select_device, generic_collate_fn\n",
    "from mugato.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7985ced-942a-45b6-a15f-2aa63999f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = data_home / \"out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861caa9-23ac-450f-a243-4a9e8de64df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc94fb-8dab-4c96-8b8f-7795da09c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 512\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "\n",
    "block_size=768\n",
    "batch_size=4\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09b1c7-35cd-43ca-a24b-9068da015ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "tokenizer = Tokenizer(text_tokenizer)\n",
    "train_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"train\", block_size=block_size))\n",
    "val_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"val\", block_size=block_size))\n",
    "test_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"test\", block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf51e28-6b8d-4099-a27f-6fbbe359ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "transformer_model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    bias=bias,\n",
    "    vocab_size=50257,  # tiktoken.get_encoding(\"r50k_base\").n_vocab\n",
    "    dropout=dropout,\n",
    ")  # start with model_args from command line\n",
    "\n",
    "mugato_model_args = dict(\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    vocab_size=51281,  # text vocab + discrete vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150a25a-67c6-43b4-9281-e78459805552",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "# force these config attributes to be equal otherwise we can't even resume training\n",
    "# the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "    transformer_model_args[k] = checkpoint_model_args[k]\n",
    "# create the model\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "model = Mugato(tokenizer, transformer, mugato_config)\n",
    "state_dict = checkpoint[\"model\"]\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "iter_num = checkpoint[\"iter_num\"]\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b1d3e-b3b6-43ef-8d5e-ecd92ce103eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0e788-1c92-4fde-b004-a1b83254ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586a42e-e03b-43d9-84e2-bc765da8060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "logits, loss = model(xs, pad=False)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import (\n",
    "    initialize as initialize_four_rooms, \n",
    "    create_dataloader as create_four_rooms_dataloader, \n",
    "    tokenize as four_rooms_tokenize\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba69a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_rooms_dataset = initialize_four_rooms()\n",
    "four_rooms_dataloader = create_four_rooms_dataloader(tokenizer, batch_size=batch_size, split=\"test\")\n",
    "batch = next(iter(four_rooms_dataloader))\n",
    "X, Y, M = batch\n",
    "X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
    "logits, loss = model(X, Y, M)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820997",
   "metadata": {},
   "source": [
    "# Test Four Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = four_rooms_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9707593",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import four_rooms_to_rgb\n",
    "from mugato.utils import image_transform\n",
    "from mugato.utils import Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(obs):\n",
    "    mission_tokens = [\n",
    "        tokenizer.encode_text(mission)\n",
    "        for mission in obs[\"mission\"]\n",
    "    ]\n",
    "    direction_tokens = [\n",
    "        tokenizer.encode_discrete([direction])\n",
    "        for direction in obs[\"direction\"]\n",
    "    ]\n",
    "    _image = obs[\"image\"]\n",
    "    _image = four_rooms_to_rgb(_image)\n",
    "    image_tokens = [tokenizer.encode_image(image) for image in image_transform(_image)]\n",
    "    action_tokens = [\n",
    "        tokenizer.encode_discrete([tokenizer.separator, action])\n",
    "        for action in obs[\"action\"]\n",
    "    ]\n",
    "\n",
    "    mission = torch.stack(mission_tokens)\n",
    "    direction = torch.stack(direction_tokens)\n",
    "    image = torch.stack(image_tokens)\n",
    "    action = torch.stack(action_tokens)\n",
    "    xs = Timesteps({\n",
    "        \"mission\": mission,\n",
    "        \"direction\": direction,\n",
    "        \"image\": image,\n",
    "        \"action\": action,\n",
    "    })\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32645aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = tokenize(obs)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_four_rooms(embedder, xs, ys=None, ms=None, sequence_length=1024, pad=True):\n",
    "    embeddings = torch.concat([embedder.embed(v) for k, v in xs.items()], dim=2)\n",
    "    B, E, T, C = embeddings.shape\n",
    "    embeddings = embeddings.view(B, E * T, C)\n",
    "    # Slice off final actions, so we can predict it.\n",
    "    return embeddings[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ce3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8768bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(next_token[0])\n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(device).unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    next_word_token = None\n",
    "    i = 1\n",
    "    xs = xs.to(device)\n",
    "    # Predict the next action. Use our custom sequence function that will slice off the final action (our temporary dummy action).\n",
    "    logits, loss = model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    next_token\n",
    "    terminated, truncated, info, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea0034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, truncated, info = env.step(next_token[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaabc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])\n",
    "xs_new = tokenize(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the new episode.\n",
    "xs = Timesteps([\n",
    "    (k, torch.concat([xs[k], xs_new[k].to(device).unsqueeze(0)])) for k in xs.keys()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08976bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 1\n",
    "xs = xs.to(device)\n",
    "logits, loss = model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs['mission'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208d66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b60f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5010a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645895e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620c773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2f8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29cd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58c0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba04568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed5340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence(embedder, xs, pad=False):\n",
    "    # xs == Timesteps({\n",
    "    #     \"mission\": (B, E, T, C),\n",
    "    #     \"direction\": (B, E, T, C),\n",
    "    #     \"image\": (B, E, T, C),\n",
    "    #     \"action\": (B, E, T, C),\n",
    "    # })\n",
    "    values = list(xs.values())\n",
    "    n_episodes = values[0].size(1)\n",
    "    # Concat all but the last episode.\n",
    "    # The final episode's \n",
    "    embeddings = torch.concat([embedder.embed(v[:, :-1]) for v in values], dim=2)\n",
    "    observations = values[:-1]\n",
    "    actions = values[-1:]\n",
    "    embeddings = torch.concat([embedder.embed(observations)], dim=2)\n",
    "    B, E, T, C = embeddings.shape\n",
    "    embeddings = embeddings.view(B, E * T, C)\n",
    "    return embeddings, observations, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_tokens = [\n",
    "    tokenizer.encode_text(mission)\n",
    "    for mission in obs[\"mission\"]\n",
    "]\n",
    "direction_tokens = [\n",
    "    tokenizer.encode_discrete([direction])\n",
    "    for direction in obs[\"direction\"]\n",
    "]\n",
    "_image = obs[\"image\"]\n",
    "_image = four_rooms_to_rgb(_image)\n",
    "image_tokens = [tokenizer.encode_image(image) for image in image_transform(_image)]\n",
    "# Add the new action token\n",
    "action_tokens = [tokenizer.encode_discrete([next_token[0], tokenizer.separator])]\n",
    "\n",
    "mission = torch.stack(mission_tokens).to(device).unsqueeze(0)\n",
    "direction = torch.stack(direction_tokens).to(device).unsqueeze(0)\n",
    "image = torch.stack(image_tokens).to(device).unsqueeze(0)\n",
    "action = torch.stack(action_tokens).to(device).unsqueeze(0)\n",
    "\n",
    "xs = Timesteps({\n",
    "    \"mission\": torch.concat([xs[\"mission\"], mission]),\n",
    "    \"direction\": torch.concat([xs[\"direction\"], direction]),\n",
    "    \"image\": torch.concat([xs[\"image\"], image]),\n",
    "    \"action\": action,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bf496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(_image[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_rooms_tokenize(tokenizer, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ff44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44066280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570fc6f-6fca-4941-9158-478cb478b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.utils import create_combined_dataloader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf097b-d47c-4d53-92c1-f668807fcce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "tokenizer = Tokenizer(text_tokenizer)\n",
    "train_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"train\", block_size=block_size))\n",
    "val_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"val\", block_size=block_size))\n",
    "test_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"test\", block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992876df-f0c2-433a-a04f-1d72dd3d7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, device):\n",
    "    if split == \"train\":\n",
    "        X, Y, M = next(next(train_dataloader))\n",
    "    elif split == \"val\":\n",
    "        X, Y, M = next(next(val_dataloader))\n",
    "    elif split == \"test\":\n",
    "        X, Y, M = next(next(test_dataloader))\n",
    "    X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
    "    return X, Y, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207aceb6-d2b6-4c29-8487-d88e45516ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8677580-a9f3-4232-baec-d30b4613f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = \"cuda\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71389aa-287a-47b9-98b5-567bee65539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9123cb-b7ec-4fd4-a939-dc98dc05633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a69dd-9f03-4f57-8078-e03176dde8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in tqdm(range(eval_iters)):\n",
    "            X, Y, M = get_batch(split, device)  # TODO: *Must* I return masks in get batch? Why?\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y, M)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae5a0b-b2a1-4335-b2dc-0591e9e7f5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3580b6a-08a5-49d8-b059-39aac0d17cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864acd5e-4587-4210-a73b-0d76aafd5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.shakespeare import initialize, create_dataloader\n",
    "from mugato.tokenizer import Tokenizer\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d299cc-99dc-46ec-a18e-913c63bffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataloader = create_dataloader(tokenizer, batch_size=batch_size, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84104a0-5b0b-4e8c-9e41-a480c1b72493",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c8bd2-89bf-4dc8-ae16-06aab606be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, M = batch\n",
    "X, Y, M = X.to(device), Y.to(device), M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edee894-01d0-4ce9-bbeb-f1c397b8ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(X, Y, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791eb38-5185-4510-90d4-14b89b555043",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c1725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa04900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bad9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

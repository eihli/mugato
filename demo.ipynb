{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2b4b6-102b-4578-9360-4c1600ad9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from mugato.data.utils import create_combined_dataloader\n",
    "from mugato.mugato import MugatoConfig, Mugato, TransformerConfig\n",
    "from mugato.nano_gpt import Block\n",
    "from mugato.utils import data_home, select_device, generic_collate_fn\n",
    "from mugato.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc94fb-8dab-4c96-8b8f-7795da09c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 512\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "block_size=768\n",
    "batch_size=4\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09b1c7-35cd-43ca-a24b-9068da015ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `create_combined_dataloader` will return a dataloader that cycles\n",
    "# through all datasets and yields a batch from each one on each iteration.\n",
    "text_tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "tokenizer = Tokenizer(text_tokenizer)\n",
    "train_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"train\", block_size=block_size))\n",
    "val_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"val\", block_size=block_size))\n",
    "test_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"test\", block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafef35-b094-4a3f-8ddb-f8e86d4c07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokens are encoded directly by the text tokenizer.\n",
    "# If the text tokenizer encodes a string as [15496, 11, 995, 0],\n",
    "# then the tokenizer will encode it as [[15496, 11, 995, 0]]\n",
    "print(tokenizer.text_tokenizer.encode(\"Hello, world!\"))\n",
    "print(tokenizer.text_tokenizer.decode(tokenizer.text_tokenizer.encode(\"Hello, world!\")))\n",
    "print(tokenizer.encode_text(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55307d4-b1ce-4d56-af4b-9ffced79baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete (and continuous, which we'll get to later) are encoded to \n",
    "# the 1024 token positions immediately after the text tokens.\n",
    "text_tokenizer.n_vocab, tokenizer.n_text, tokenizer.n_discrete, tokenizer.decode_text(torch.tensor([[50256]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5ff9d-72e3-4091-8398-1d7259cf353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first discrete token, 0, gets encoded immedately after the last text token.\n",
    "tokenizer.encode_discrete(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711e3e4-55d8-44d3-89f5-53944b875465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode_discrete([0, 1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf51e28-6b8d-4099-a27f-6fbbe359ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    bias=bias,\n",
    "    vocab_size=50257,  # tiktoken.get_encoding(\"r50k_base\").n_vocab\n",
    "    dropout=dropout,\n",
    ")  # start with model_args from command line\n",
    "\n",
    "mugato_model_args = dict(\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    vocab_size=51281,  # text vocab + discrete vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150a25a-67c6-43b4-9281-e78459805552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "untrained_model = Mugato(tokenizer, transformer, mugato_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b1d3e-b3b6-43ef-8d5e-ecd92ce103eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model = untrained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d58431-7367-45ef-9aac-c2f560900712",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0e788-1c92-4fde-b004-a1b83254ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586a42e-e03b-43d9-84e2-bc765da8060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "with torch.no_grad():\n",
    "    logits, loss = untrained_model(xs, pad=False)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import (\n",
    "    initialize as initialize_four_rooms, \n",
    "    create_dataloader as create_four_rooms_dataloader, \n",
    "    tokenize as four_rooms_tokenize\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba69a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_rooms_dataset = initialize_four_rooms()\n",
    "four_rooms_dataloader = create_four_rooms_dataloader(tokenizer, batch_size=batch_size, split=\"test\")\n",
    "batch = next(iter(four_rooms_dataloader))\n",
    "X, Y, M = batch\n",
    "X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
    "logits, loss = untrained_model(X, Y, M)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e820997",
   "metadata": {},
   "source": [
    "# Test Four Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = four_rooms_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9707593",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import four_rooms_to_rgb\n",
    "from mugato.utils import image_transform\n",
    "from mugato.utils import Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(obs):\n",
    "    mission_tokens = [\n",
    "        tokenizer.encode_text(mission)\n",
    "        for mission in obs[\"mission\"]\n",
    "    ]\n",
    "    direction_tokens = [\n",
    "        tokenizer.encode_discrete([direction])\n",
    "        for direction in obs[\"direction\"]\n",
    "    ]\n",
    "    _image = obs[\"image\"]\n",
    "    _image = four_rooms_to_rgb(_image)\n",
    "    image_tokens = [tokenizer.encode_image(image) for image in image_transform(_image)]\n",
    "    action_tokens = [\n",
    "        tokenizer.encode_discrete([tokenizer.separator, action])\n",
    "        for action in obs[\"action\"]\n",
    "    ]\n",
    "\n",
    "    mission = torch.stack(mission_tokens)\n",
    "    direction = torch.stack(direction_tokens)\n",
    "    image = torch.stack(image_tokens)\n",
    "    action = torch.stack(action_tokens)\n",
    "    xs = Timesteps({\n",
    "        \"mission\": mission,\n",
    "        \"direction\": direction,\n",
    "        \"image\": image,\n",
    "        \"action\": action,\n",
    "    })\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32645aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = tokenize(obs)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_four_rooms(embedder, xs, ys=None, ms=None, sequence_length=1024, pad=True):\n",
    "    embeddings = torch.concat([embedder.embed(v) for k, v in xs.items()], dim=2)\n",
    "    B, E, T, C = embeddings.shape\n",
    "    embeddings = embeddings.view(B, E * T, C)\n",
    "    # Slice off final actions, so we can predict it.\n",
    "    return embeddings[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ce3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = untrained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(token, action_space):\n",
    "    return token % tokenizer.n_text % env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3313589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track memory usage\n",
    "import gc\n",
    "import torch.cuda\n",
    "\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n",
    "\n",
    "print(\"Initial GPU memory:\")\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8768bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # Clear memory before each iteration\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(get_action(next_token[0], env))\n",
    "    \n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    \n",
    "    # Move old tensors to CPU to free GPU memory\n",
    "    xs = xs.to(\"cpu\")\n",
    "    \n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(\"cpu\").unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    \n",
    "    # Only move to GPU right before model inference\n",
    "    xs = xs.to(device)\n",
    "    \n",
    "    # Predict the next action\n",
    "    with torch.no_grad():  # Use mixed precision to reduce memory\n",
    "        logits, loss = untrained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    \n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    \n",
    "    # Move tensors back to CPU and clear GPU cache\n",
    "    xs = xs.to(\"cpu\")\n",
    "    logits = logits.to(\"cpu\")\n",
    "    probs = probs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4406218",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5ca7c",
   "metadata": {},
   "source": [
    "# Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4355fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = data_home / \"out\"\n",
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "state_dict = checkpoint[\"model\"]\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "    transformer_model_args[k] = checkpoint_model_args[k]\n",
    "\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "trained_model = Mugato(tokenizer, transformer, mugato_config)\n",
    "trained_model.load_state_dict(state_dict)\n",
    "iter_num = checkpoint[\"iter_num\"]\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "\n",
    "trained_model = trained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff345b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb989fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "logits, loss = trained_model(xs, pad=False)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86daa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])\n",
    "xs = tokenize(obs)\n",
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = trained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # Clear memory before each iteration\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(get_action(next_token[0], env))\n",
    "    \n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    \n",
    "    # Move old tensors to CPU to free GPU memory\n",
    "    xs = xs.to(\"cpu\")\n",
    "    \n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(\"cpu\").unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    \n",
    "    # Only move to GPU right before model inference\n",
    "    xs = xs.to(device)\n",
    "    \n",
    "    # Predict the next action\n",
    "    with torch.no_grad():  # Use mixed precision to reduce memory\n",
    "        logits, loss = trained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    \n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    \n",
    "    # Move tensors back to CPU and clear GPU cache\n",
    "    xs = xs.to(\"cpu\")\n",
    "    logits = logits.to(\"cpu\")\n",
    "    probs = probs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc18534",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41334491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161905e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208d66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b60f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5010a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645895e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620c773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2f8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29cd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58c0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba04568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed5340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

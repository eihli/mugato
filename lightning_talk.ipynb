{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41d2b4b6-102b-4578-9360-4c1600ad9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from mugato.data.utils import create_combined_dataloader\n",
    "from mugato.mugato import MugatoConfig, Mugato, TransformerConfig\n",
    "from mugato.nano_gpt import Block\n",
    "from mugato.utils import data_home, select_device, generic_collate_fn\n",
    "from mugato.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c9fc94fb-8dab-4c96-8b8f-7795da09c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 512\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "block_size=768\n",
    "batch_size=4\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b09b1c7-35cd-43ca-a24b-9068da015ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "tokenizer = Tokenizer(text_tokenizer)\n",
    "\n",
    "# Ask me about \"combined dataloaders\".\n",
    "train_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"train\", block_size=block_size))\n",
    "val_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"val\", block_size=block_size))\n",
    "test_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"test\", block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8c077-eeba-48a6-becf-3b7daff974f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b7b159-107a-4b79-9bea-96bbec1f0494",
   "metadata": {},
   "source": [
    "Let's review how we tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5aa5687a-9aff-4586-9a0b-bdb295ccec17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, abcDEF world!'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"Hello, abcDEF world!\"\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a26db3-3ea6-42bb-875f-96abbec3e179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4aafef35-b094-4a3f-8ddb-f8e86d4c07bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 450, 66, 32988, 995, 0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.text_tokenizer.encode(\"Hello, abcDEF world!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60293a9-8393-4bb0-9c78-72462ea399f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee9488ec-85dd-4dc4-8463-402fcd99b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, abcDEF world!'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.text_tokenizer.decode(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49c2235b-d5fe-447a-8884-6707b9a5e775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text == decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00795a4-3cd2-492c-bf86-790f323dc286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad5e6928-c325-4934-81dd-43b862a569b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token:       text\n",
      "     15496:      Hello\n",
      "        11:          ,\n",
      "       450:         ab\n",
      "        66:          c\n",
      "     32988:        DEF\n",
      "       995:      world\n",
      "         0:          !\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"token\":>10}: {\"text\":>10}')\n",
    "\n",
    "for token in tokenizer.text_tokenizer.encode(\"Hello, abcDEF world!\"):\n",
    "    decoded = tokenizer.text_tokenizer.decode([token])\n",
    "    print(f'{token:>10}: {decoded:>10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e08de383-c15f-4100-aeb6-878ab6d4ec15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.decode([text_tokenizer.eot_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8327d-e22b-4381-b1c1-5b9d5bf61181",
   "metadata": {},
   "source": [
    "Now let's talk about how we'll tokenize \"discrete actions\", like pressing \"up\" or \"b\" on an Atari controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77ac4b3c-40e7-4b25-9abd-b170bbf34f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0443e0-1509-4da5-a091-2329e748d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a322c33-4787-4aa2-854f-16e245273ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b55307d4-b1ce-4d56-af4b-9ffced79baba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2895fa-db25-4185-983c-094b9d5458c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eba2f70-dc84-4d71-89b0-e89366c43681",
   "metadata": {},
   "source": [
    "If `1` represents \"down\" on an Atari controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fd5ff9d-72e3-4091-8398-1d7259cf353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50258])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_discrete(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3996c-d292-4d61-9d02-f30e0a972b36",
   "metadata": {},
   "source": [
    "We encode discrete variables just beyond the range of our text encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897c785-dfa9-49ec-8951-e8001379ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5711e3e4-55d8-44d3-89f5-53944b875465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50257])\n",
      "tensor([50258])\n",
      "tensor([50259])\n",
      "tensor([50260])\n",
      "tensor([50261])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(tokenizer.encode_discrete(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a252a2-f18a-43db-bcd7-e10f49f490e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d0a2d74-c28b-49d1-82f0-ff210c35c2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_text(torch.tensor([[50256]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b18a99-3b18-4e8b-a9d5-c580a0723e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657673a8-a67c-47e3-ab1d-fe9de9670ceb",
   "metadata": {},
   "source": [
    "Now let's create our model. Then we'll look at how we turn those tokens into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137c56e-e942-45b7-b547-6d858d22c953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "caf51e28-6b8d-4099-a27f-6fbbe359ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "transformer_model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    bias=bias,\n",
    "    vocab_size=50257,  # tiktoken.get_encoding(\"r50k_base\").n_vocab\n",
    "    dropout=dropout,\n",
    ")  # start with model_args from command line\n",
    "\n",
    "mugato_model_args = dict(\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    vocab_size=51281,  # text vocab + discrete vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f150a25a-67c6-43b4-9281-e78459805552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "untrained_model = Mugato(tokenizer, transformer, mugato_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b68b1d3e-b3b6-43ef-8d5e-ecd92ce103eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model = untrained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6865710-feab-4b3f-9c13-df8882eee4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2d58431-7367-45ef-9aac-c2f560900712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mugato(\n",
      "  (lookup_embedding): Embedding(51281, 512)\n",
      "  (image_embedding): ResNetV2(\n",
      "    (stem): Sequential(\n",
      "      (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "print(str(untrained_model)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfc8cc-f810-4b48-a19e-74780c1465bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db8d6a8-21d9-41c2-96f7-aa1617f0bb79",
   "metadata": {},
   "source": [
    "Let's see how our untrained model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ff7c4-6105-4dfc-a043-cb1f5ab6467b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36c0e788-1c92-4fde-b004-a1b83254ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "03e83344-a942-4f1a-be18-0be95799b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[50256],\n",
       "         [ 5962],\n",
       "         [22307],\n",
       "         [   25],\n",
       "         [  198]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7586a42e-e03b-43d9-84e2-bc765da8060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "yll upload healUsage Posterliving� LAST Tong\n"
     ]
    }
   ],
   "source": [
    "# You can ignore the next few lines. Happy to go into it, but it's technical implementation details.\n",
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "\n",
    "# This is where we send our tokens through our model and get back \"logits\".\n",
    "# Logits are like the \"score\" of how good each word is (how well it fits given the previous text).\n",
    "with torch.no_grad():\n",
    "    logits, loss = untrained_model(xs, pad=False)\n",
    "\n",
    "# Temperature controls how \"flat\" is the probability distribution over the possible next words.\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "\n",
    "# Select the word based on the probabilities above\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e8596-5689-48f2-9f95-39e59c67c6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "398aa8c2-e1f8-4156-8df9-97c6c6093c00",
   "metadata": {},
   "source": [
    "It's completely random nonsense, as expected. At least it doesn't throw exceptions! (A pleasant surprise!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b90b9e-04b4-422a-9797-0149d9f13650",
   "metadata": {},
   "source": [
    "Let's grab a single sample so and try to train on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "624e94de-9059-4fde-998f-883e4ed4ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b1eb7-a92a-4922-84cb-8ae81ab49332",
   "metadata": {},
   "source": [
    "Let's check out how it works with a robotic dataset that combines text, image, and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2a001-6839-40ec-9190-2ffa2954cd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d81f0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import (\n",
    "    initialize as initialize_four_rooms, \n",
    "    create_dataloader as create_four_rooms_dataloader, \n",
    "    tokenize as four_rooms_tokenize\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fba69a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_rooms_dataset = initialize_four_rooms()\n",
    "four_rooms_dataloader = create_four_rooms_dataloader(tokenizer, batch_size=batch_size, split=\"test\")\n",
    "batch = next(iter(four_rooms_dataloader))\n",
    "X, Y, M = batch\n",
    "X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
    "logits, loss = untrained_model(X, Y, M)\n",
    "\n",
    "# Before we print the \"loss\" here, what do you expect it to be?\n",
    "\n",
    "# The \"loss\" is how far away we are from our prediction.\n",
    "\n",
    "# How many classes do we have? How many possible output tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "15ca0f7b-2b53-4e6a-b974-11b259f3d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51281"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_tokens = tokenizer.n_text + tokenizer.n_discrete\n",
    "total_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c8a8e8c7-a9ba-47d3-8cf0-f78d6bb857e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.845075592184445"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-math.log(1/51281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1589e4e0-8202-4936-9b98-30532eaaef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4766, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e701b7-5fb0-4674-bcf0-1f5f253be102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a completely random model, what do we expect the probability of any given token to be, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95373b-004a-48bb-a876-f0fc2d90ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e820997",
   "metadata": {},
   "source": [
    "# Test Four Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "352786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = four_rooms_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9707593",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "732e3434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "glx: failed to create dri3 screen\n",
      "failed to load driver: nouveau\n",
      "glx: failed to create dri3 screen\n",
      "failed to load driver: nouveau\n"
     ]
    }
   ],
   "source": [
    "# TODO: Render mode that works in Jupyter notebook.\n",
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9fccecb8-e276-4333-9a5d-0bd270161a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvCheckerWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWG\n",
       "WG    GG          WG                WG\n",
       "WG                                  WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG              >>WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WGWGWG  WGWGWGWGWGWGWGWGWGWG  WGWGWGWG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                                  WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWG>>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46cf9664-ef05-4af2-8b1a-ad1f18139120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [2, 5, 0]]]], dtype=uint8),\n",
       " 'direction': array([0]),\n",
       " 'mission': ['reach the goal'],\n",
       " 'action': array([0])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b576cd0a-bd13-43b1-8ef3-8cea145096e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7c9618230ad0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs['image'][0].astype(np.float32) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "83a9bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import four_rooms_to_rgb\n",
    "from mugato.utils import image_transform\n",
    "from mugato.utils import Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9b7417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "228fa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(obs):\n",
    "    # Separately encode the mission text, the discrete direction, and the image observation.\n",
    "    \n",
    "    mission_tokens = [\n",
    "        tokenizer.encode_text(mission)\n",
    "        for mission in obs[\"mission\"]\n",
    "    ]\n",
    "    direction_tokens = [\n",
    "        tokenizer.encode_discrete([direction])\n",
    "        for direction in obs[\"direction\"]\n",
    "    ]\n",
    "    _image = obs[\"image\"]\n",
    "    _image = four_rooms_to_rgb(_image)\n",
    "    image_tokens = [tokenizer.encode_image(image) for image in image_transform(_image)]\n",
    "    action_tokens = [\n",
    "        tokenizer.encode_discrete([tokenizer.separator, action])\n",
    "        for action in obs[\"action\"]\n",
    "    ]\n",
    "\n",
    "    mission = torch.stack(mission_tokens)\n",
    "    direction = torch.stack(direction_tokens)\n",
    "    image = torch.stack(image_tokens)\n",
    "    action = torch.stack(action_tokens)\n",
    "    xs = Timesteps({\n",
    "        \"mission\": mission,\n",
    "        \"direction\": direction,\n",
    "        \"image\": image,\n",
    "        \"action\": action,\n",
    "    })\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "32645aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timesteps([('mission',\n",
       "            tensor([[[16250],\n",
       "                     [  262],\n",
       "                     [ 3061]]])),\n",
       "           ('direction', tensor([[[50257]]])),\n",
       "           ('image',\n",
       "            tensor([[[ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     [ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     [ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     ...,\n",
       "                     [ 0.1859,  0.1965,  0.2072,  ..., -0.1753, -0.1678, -0.1603],\n",
       "                     [ 0.2500,  0.2341,  0.2181,  ..., -0.0571, -0.0459, -0.0347],\n",
       "                     [ 0.2393,  0.1374,  0.0573,  ...,  0.2500,  0.2500,  0.2500]]])),\n",
       "           ('action',\n",
       "            tensor([[[51280],\n",
       "                     [50257]]]))])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = tokenize(obs)\n",
    "# Notice that the image isn't really a \"token\" in the same sense as text/discrete are.\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9acd7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "982d72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_four_rooms(embedder, xs, ys=None, ms=None, sequence_length=1024, pad=True):\n",
    "    # Refer to the Gato paper for the sequencing diagram.\n",
    "    \n",
    "    embeddings = torch.concat([embedder.embed(v) for k, v in xs.items()], dim=2)\n",
    "    B, E, T, C = embeddings.shape\n",
    "    embeddings = embeddings.view(B, E * T, C)\n",
    "    # Slice off final actions, so we can predict it.\n",
    "    return embeddings[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5c2ce3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = untrained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6fd4c229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16266]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the \"action\" token that we're predicting.\n",
    "# It's just as gibberish as the text we were generating before.\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3286423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(token, action_space):\n",
    "    # This is just how we turn any arbitrary wrong action token into a valid (but wrong) Gym environment action.\n",
    "    # Our code would blow up if we told our Gym environment to take action 16266. That's out of\n",
    "    # the range of valid actions.\n",
    "    return token % tokenizer.n_text % env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8a4ad7d6-0c16-44e8-9193-b6914ed5d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(5)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_action(next_token[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3313589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory:\n",
      "Allocated: 10.67GB\n",
      "Cached: 11.58GB\n"
     ]
    }
   ],
   "source": [
    "# Track memory usage\n",
    "import gc\n",
    "import torch.cuda\n",
    "\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n",
    "\n",
    "print(\"Initial GPU memory:\")\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d8768bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [46182]\n",
      "Next token: [9841]\n",
      "Next token: [8374]\n",
      "Next token: [30912]\n",
      "Next token: [5240]\n",
      "Next token: [20185]\n",
      "Next token: [27804]\n",
      "Next token: [3663]\n",
      "Next token: [32419]\n",
      "Next token: [34277]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Clear memory before each iteration\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(get_action(next_token[0], env))\n",
    "    \n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    \n",
    "    # Move old tensors to CPU to free GPU memory\n",
    "    xs = xs.to(\"cpu\")\n",
    "    \n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(\"cpu\").unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    \n",
    "    # Only move to GPU right before model inference\n",
    "    xs = xs.to(device)\n",
    "    \n",
    "    # Predict the next action\n",
    "    with torch.no_grad():  # Use mixed precision to reduce memory\n",
    "        logits, loss = untrained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    \n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    \n",
    "    # Move tensors back to CPU and clear GPU cache\n",
    "    xs = xs.to(\"cpu\")\n",
    "    logits = logits.to(\"cpu\")\n",
    "    probs = probs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e4406218",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "afea0034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5ca7c",
   "metadata": {},
   "source": [
    "# Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d4355fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = data_home / \"out\"\n",
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "state_dict = checkpoint[\"model\"]\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "    transformer_model_args[k] = checkpoint_model_args[k]\n",
    "\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "trained_model = Mugato(tokenizer, transformer, mugato_config)\n",
    "trained_model.load_state_dict(state_dict)\n",
    "iter_num = checkpoint[\"iter_num\"]\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "\n",
    "trained_model = trained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ff345b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bb989fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Right, good word.\n",
      "\n",
      "!\n",
      "\n",
      " word, good old!\n",
      "\n",
      " word,\n",
      "Not to the poor souls,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "logits, loss = trained_model(xs, pad=False)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "86daa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])\n",
    "xs = tokenize(obs)\n",
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f825748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = trained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5b0e8182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [1]\n",
      "Next token: [1]\n",
      "Next token: [0]\n",
      "Next token: [2]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Clear memory before each iteration\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(get_action(next_token[0], env))\n",
    "    \n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    \n",
    "    # Move old tensors to CPU to free GPU memory\n",
    "    xs = xs.to(\"cpu\")\n",
    "    \n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(\"cpu\").unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    \n",
    "    # Only move to GPU right before model inference\n",
    "    xs = xs.to(device)\n",
    "    \n",
    "    # Predict the next action\n",
    "    with torch.no_grad():  # Use mixed precision to reduce memory\n",
    "        logits, loss = trained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    \n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    \n",
    "    # Move tensors back to CPU and clear GPU cache\n",
    "    xs = xs.to(\"cpu\")\n",
    "    logits = logits.to(\"cpu\")\n",
    "    probs = probs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fc18534",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af869a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f634b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

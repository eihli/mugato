{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d2b4b6-102b-4578-9360-4c1600ad9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from mugato.data.utils import create_combined_dataloader\n",
    "from mugato.mugato import MugatoConfig, Mugato, TransformerConfig\n",
    "from mugato.nano_gpt import Block\n",
    "from mugato.utils import data_home, select_device, generic_collate_fn\n",
    "from mugato.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fc94fb-8dab-4c96-8b8f-7795da09c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6\n",
    "n_head = 4\n",
    "n_embd = 512\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "block_size=768\n",
    "batch_size=4\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b09b1c7-35cd-43ca-a24b-9068da015ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "tokenizer = Tokenizer(text_tokenizer)\n",
    "\n",
    "# Ask me about \"combined dataloaders\".\n",
    "train_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"train\", block_size=block_size))\n",
    "val_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"val\", block_size=block_size))\n",
    "test_dataloader = iter(create_combined_dataloader(tokenizer, batch_size, split=\"test\", block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8c077-eeba-48a6-becf-3b7daff974f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b7b159-107a-4b79-9bea-96bbec1f0494",
   "metadata": {},
   "source": [
    "Let's review how we tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa5687a-9aff-4586-9a0b-bdb295ccec17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, abcDEF world!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"Hello, abcDEF world!\"\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a26db3-3ea6-42bb-875f-96abbec3e179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aafef35-b094-4a3f-8ddb-f8e86d4c07bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 450, 66, 32988, 995, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.text_tokenizer.encode(\"Hello, abcDEF world!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60293a9-8393-4bb0-9c78-72462ea399f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9488ec-85dd-4dc4-8463-402fcd99b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, abcDEF world!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.text_tokenizer.decode(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c2235b-d5fe-447a-8884-6707b9a5e775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text == decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00795a4-3cd2-492c-bf86-790f323dc286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5e6928-c325-4934-81dd-43b862a569b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token:       text\n",
      "     15496:      Hello\n",
      "        11:          ,\n",
      "       450:         ab\n",
      "        66:          c\n",
      "     32988:        DEF\n",
      "       995:      world\n",
      "         0:          !\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"token\":>10}: {\"text\":>10}')\n",
    "\n",
    "for token in tokenizer.text_tokenizer.encode(\"Hello, abcDEF world!\"):\n",
    "    decoded = tokenizer.text_tokenizer.decode([token])\n",
    "    print(f'{token:>10}: {decoded:>10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08de383-c15f-4100-aeb6-878ab6d4ec15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.decode([text_tokenizer.eot_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8327d-e22b-4381-b1c1-5b9d5bf61181",
   "metadata": {},
   "source": [
    "Now let's talk about how we'll tokenize \"discrete actions\", like pressing \"up\" or \"b\" on an Atari controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ac4b3c-40e7-4b25-9abd-b170bbf34f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0443e0-1509-4da5-a091-2329e748d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a322c33-4787-4aa2-854f-16e245273ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55307d4-b1ce-4d56-af4b-9ffced79baba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2895fa-db25-4185-983c-094b9d5458c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eba2f70-dc84-4d71-89b0-e89366c43681",
   "metadata": {},
   "source": [
    "If `1` represents \"down\" on an Atari controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd5ff9d-72e3-4091-8398-1d7259cf353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50258])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_discrete(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3996c-d292-4d61-9d02-f30e0a972b36",
   "metadata": {},
   "source": [
    "We encode discrete variables just beyond the range of our text encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897c785-dfa9-49ec-8951-e8001379ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5711e3e4-55d8-44d3-89f5-53944b875465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50257])\n",
      "tensor([50258])\n",
      "tensor([50259])\n",
      "tensor([50260])\n",
      "tensor([50261])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(tokenizer.encode_discrete(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a252a2-f18a-43db-bcd7-e10f49f490e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d0a2d74-c28b-49d1-82f0-ff210c35c2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_text(torch.tensor([[50256]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b18a99-3b18-4e8b-a9d5-c580a0723e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657673a8-a67c-47e3-ab1d-fe9de9670ceb",
   "metadata": {},
   "source": [
    "Now let's create our model. Then we'll look at how we turn those tokens into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137c56e-e942-45b7-b547-6d858d22c953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf51e28-6b8d-4099-a27f-6fbbe359ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "transformer_model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    bias=bias,\n",
    "    vocab_size=50257,  # tiktoken.get_encoding(\"r50k_base\").n_vocab\n",
    "    dropout=dropout,\n",
    ")  # start with model_args from command line\n",
    "\n",
    "mugato_model_args = dict(\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    vocab_size=51281,  # text vocab + discrete vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f150a25a-67c6-43b4-9281-e78459805552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "untrained_model = Mugato(tokenizer, transformer, mugato_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b68b1d3e-b3b6-43ef-8d5e-ecd92ce103eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model = untrained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6865710-feab-4b3f-9c13-df8882eee4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2d58431-7367-45ef-9aac-c2f560900712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mugato(\n",
      "  (lookup_embedding): Embedding(51281, 512)\n",
      "  (image_embedding): ResNetV2(\n",
      "    (stem): Sequential(\n",
      "      (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "print(str(untrained_model)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfc8cc-f810-4b48-a19e-74780c1465bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db8d6a8-21d9-41c2-96f7-aa1617f0bb79",
   "metadata": {},
   "source": [
    "Let's see how our untrained model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ff7c4-6105-4dfc-a043-cb1f5ab6467b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36c0e788-1c92-4fde-b004-a1b83254ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03e83344-a942-4f1a-be18-0be95799b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[50256],\n",
       "         [ 5962],\n",
       "         [22307],\n",
       "         [   25],\n",
       "         [  198]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7586a42e-e03b-43d9-84e2-bc765da8060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "uggets steer listensÃŸXY Wytypes disruption NoticecodedThose LightingocketsCong estate\n"
     ]
    }
   ],
   "source": [
    "# You can ignore the next few lines. Happy to go into it, but it's technical implementation details.\n",
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "\n",
    "# This is where we send our tokens through our model and get back \"logits\".\n",
    "# Logits are like the \"score\" of how good each word is (how well it fits given the previous text).\n",
    "with torch.no_grad():\n",
    "    logits, loss = untrained_model(xs, pad=False)\n",
    "\n",
    "# Temperature controls how \"flat\" is the probability distribution over the possible next words.\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "\n",
    "# Select the word based on the probabilities above\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e8596-5689-48f2-9f95-39e59c67c6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "398aa8c2-e1f8-4156-8df9-97c6c6093c00",
   "metadata": {},
   "source": [
    "It's completely random nonsense, as expected. At least it doesn't throw exceptions! (A pleasant surprise!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b90b9e-04b4-422a-9797-0149d9f13650",
   "metadata": {},
   "source": [
    "Let's grab a single sample so and try to train on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "624e94de-9059-4fde-998f-883e4ed4ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b1eb7-a92a-4922-84cb-8ae81ab49332",
   "metadata": {},
   "source": [
    "Let's check out how it works with a robotic dataset that combines text, image, and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2a001-6839-40ec-9190-2ffa2954cd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d81f0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import (\n",
    "    initialize as initialize_four_rooms, \n",
    "    create_dataloader as create_four_rooms_dataloader, \n",
    "    tokenize as four_rooms_tokenize\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fba69a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_rooms_dataset = initialize_four_rooms()\n",
    "four_rooms_dataloader = create_four_rooms_dataloader(tokenizer, batch_size=batch_size, split=\"test\")\n",
    "batch = next(iter(four_rooms_dataloader))\n",
    "X, Y, M = batch\n",
    "X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
    "logits, loss = untrained_model(X, Y, M)\n",
    "\n",
    "# Before we print the \"loss\" here, what do you expect it to be?\n",
    "\n",
    "# The \"loss\" is how far away we are from our prediction.\n",
    "\n",
    "# How many classes do we have? How many possible output tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15ca0f7b-2b53-4e6a-b974-11b259f3d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51281"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_tokens = tokenizer.n_text + tokenizer.n_discrete\n",
    "total_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46656c8b-514e-4db6-91fd-bc90e52700a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8a8e8c7-a9ba-47d3-8cf0-f78d6bb857e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.845075592184445"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-math.log(1/51281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1589e4e0-8202-4936-9b98-30532eaaef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0579, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9e701b7-5fb0-4674-bcf0-1f5f253be102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a completely random model, what do we expect the probability of any given token to be, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95373b-004a-48bb-a876-f0fc2d90ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e820997",
   "metadata": {},
   "source": [
    "# Test Four Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "352786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = four_rooms_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9707593",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "732e3434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "glx: failed to create dri3 screen\n",
      "failed to load driver: nouveau\n",
      "glx: failed to create dri3 screen\n",
      "failed to load driver: nouveau\n"
     ]
    }
   ],
   "source": [
    "# TODO: Render mode that works in Jupyter notebook.\n",
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fccecb8-e276-4333-9a5d-0bd270161a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvCheckerWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                                  WG\n",
       "WG  <<            WG                WG\n",
       "WGWGWGWG  WGWGWGWGWGWGWGWGWG  WGWGWGWG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WG                WG              GGWG\n",
       "WG                                  WG\n",
       "WG                WG                WG\n",
       "WG                WG                WG\n",
       "WGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWGWG>>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46cf9664-ef05-4af2-8b1a-ad1f18139120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0]],\n",
       " \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]]]], dtype=uint8),\n",
       " 'direction': array([2]),\n",
       " 'mission': ['reach the goal'],\n",
       " 'action': array([0])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b576cd0a-bd13-43b1-8ef3-8cea145096e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7da1b0312f60>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs['image'][0].astype(np.float32) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83a9bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mugato.data.four_rooms import four_rooms_to_rgb\n",
    "from mugato.utils import image_transform\n",
    "from mugato.utils import Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9b7417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "228fa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(obs):\n",
    "    # Separately encode the mission text, the discrete direction, and the image observation.\n",
    "    \n",
    "    mission_tokens = [\n",
    "        tokenizer.encode_text(mission)\n",
    "        for mission in obs[\"mission\"]\n",
    "    ]\n",
    "    direction_tokens = [\n",
    "        tokenizer.encode_discrete([direction])\n",
    "        for direction in obs[\"direction\"]\n",
    "    ]\n",
    "    _image = obs[\"image\"]\n",
    "    _image = four_rooms_to_rgb(_image)\n",
    "    image_tokens = [tokenizer.encode_image(image) for image in image_transform(_image)]\n",
    "    action_tokens = [\n",
    "        tokenizer.encode_discrete([tokenizer.separator, action])\n",
    "        for action in obs[\"action\"]\n",
    "    ]\n",
    "\n",
    "    mission = torch.stack(mission_tokens)\n",
    "    direction = torch.stack(direction_tokens)\n",
    "    image = torch.stack(image_tokens)\n",
    "    action = torch.stack(action_tokens)\n",
    "    xs = Timesteps({\n",
    "        \"mission\": mission,\n",
    "        \"direction\": direction,\n",
    "        \"image\": image,\n",
    "        \"action\": action,\n",
    "    })\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32645aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timesteps([('mission',\n",
       "            tensor([[[16250],\n",
       "                     [  262],\n",
       "                     [ 3061]]])),\n",
       "           ('direction', tensor([[[50259]]])),\n",
       "           ('image',\n",
       "            tensor([[[ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     [ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     [ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     ...,\n",
       "                     [ 0.1782,  0.1895,  0.2008,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     [ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230],\n",
       "                     [ 0.2500,  0.2500,  0.2500,  ..., -0.2230, -0.2230, -0.2230]]])),\n",
       "           ('action',\n",
       "            tensor([[[51280],\n",
       "                     [50257]]]))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = tokenize(obs)\n",
    "# Notice that the image isn't really a \"token\" in the same sense as text/discrete are.\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9acd7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "982d72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_four_rooms(embedder, xs, ys=None, ms=None, sequence_length=1024, pad=True):\n",
    "    # Refer to the Gato paper for the sequencing diagram.\n",
    "    \n",
    "    embeddings = torch.concat([embedder.embed(v) for k, v in xs.items()], dim=2)\n",
    "    B, E, T, C = embeddings.shape\n",
    "    embeddings = embeddings.view(B, E * T, C)\n",
    "    # Slice off final actions, so we can predict it.\n",
    "    return embeddings[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c2ce3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = untrained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6fd4c229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14686]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the \"action\" token that we're predicting.\n",
    "# It's just as gibberish as the text we were generating before.\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3286423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(token, action_space):\n",
    "    # This is just how we turn any arbitrary wrong action token into a valid (but wrong) Gym environment action.\n",
    "    # Our code would blow up if we told our Gym environment to take action 16266. That's out of\n",
    "    # the range of valid actions.\n",
    "    return token % tokenizer.n_text % env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a4ad7d6-0c16-44e8-9193-b6914ed5d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_action(next_token[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3313589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory:\n",
      "Allocated: 10.67GB\n",
      "Cached: 11.58GB\n"
     ]
    }
   ],
   "source": [
    "# Track memory usage\n",
    "import gc\n",
    "import torch.cuda\n",
    "\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n",
    "\n",
    "print(\"Initial GPU memory:\")\n",
    "print_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d8768bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [46182]\n",
      "Next token: [9841]\n",
      "Next token: [8374]\n",
      "Next token: [30912]\n",
      "Next token: [5240]\n",
      "Next token: [20185]\n",
      "Next token: [27804]\n",
      "Next token: [3663]\n",
      "Next token: [32419]\n",
      "Next token: [34277]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Clear memory before each iteration\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(get_action(next_token[0], env))\n",
    "    \n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    \n",
    "    # Move old tensors to CPU to free GPU memory\n",
    "    xs = xs.to(\"cpu\")\n",
    "    \n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(\"cpu\").unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    \n",
    "    # Only move to GPU right before model inference\n",
    "    xs = xs.to(device)\n",
    "    \n",
    "    # Predict the next action\n",
    "    with torch.no_grad():  # Use mixed precision to reduce memory\n",
    "        logits, loss = untrained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    \n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    \n",
    "    # Move tensors back to CPU and clear GPU cache\n",
    "    xs = xs.to(\"cpu\")\n",
    "    logits = logits.to(\"cpu\")\n",
    "    probs = probs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e4406218",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "afea0034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5ca7c",
   "metadata": {},
   "source": [
    "# Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d4355fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = data_home / \"out\"\n",
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "state_dict = checkpoint[\"model\"]\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "    transformer_model_args[k] = checkpoint_model_args[k]\n",
    "\n",
    "transformer_config = TransformerConfig(**transformer_model_args)\n",
    "transformer = nn.ModuleDict(\n",
    "    dict(\n",
    "        wpe=nn.Embedding(transformer_config.block_size, transformer_config.n_embd),\n",
    "        drop=nn.Dropout(transformer_config.dropout),\n",
    "        h=nn.ModuleList(\n",
    "            [\n",
    "                Block(transformer_config)\n",
    "                for _ in range(transformer_config.n_layer)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "mugato_config = MugatoConfig(**mugato_model_args)\n",
    "trained_model = Mugato(tokenizer, transformer, mugato_config)\n",
    "trained_model.load_state_dict(state_dict)\n",
    "iter_num = checkpoint[\"iter_num\"]\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "\n",
    "trained_model = trained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ff345b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval()\n",
    "text = \"First Citizen:\\n\"\n",
    "tokens = torch.stack([torch.concat([torch.tensor([tokenizer.eot_token_id]).unsqueeze(0), tokenizer.encode_text(text)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bb989fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Right, good word.\n",
      "\n",
      "!\n",
      "\n",
      " word, good old!\n",
      "\n",
      " word,\n",
      "Not to the poor souls,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xs = OrderedDict(text=tokens)\n",
    "xs, ys, ms = generic_collate_fn([[xs, xs]])\n",
    "next_word_token = None\n",
    "i = 0\n",
    "xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "logits, loss = trained_model(xs, pad=False)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_word_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_word = tokenizer.decode_text(next_word_token)\n",
    "text += next_word\n",
    "tokens = torch.stack([tokenizer.encode_text(text)])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "86daa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_data.recover_environment(render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "obs['direction'] = np.array([obs['direction']])\n",
    "obs['image'] = np.array([obs['image']])\n",
    "obs['mission'] = [obs['mission']]\n",
    "dummy_action = 0  # Will be sliced off after sequencing.\n",
    "obs['action'] = np.array([dummy_action])\n",
    "xs = tokenize(obs)\n",
    "# Add batch dimension.\n",
    "xs = Timesteps([\n",
    "    (k, torch.stack([v])) for k, v in xs.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f825748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_token = None\n",
    "i = 0\n",
    "xs = xs.to(device)\n",
    "logits, loss = trained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "temp = 0.6\n",
    "scaled_logits = logits / temp\n",
    "probs = scaled_logits.softmax(dim=2)\n",
    "next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "next_token = tokenizer.decode_discrete(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5b0e8182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [2]\n",
      "Next token: [1]\n",
      "Next token: [1]\n",
      "Next token: [0]\n",
      "Next token: [2]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Clear memory before each iteration\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step with the previously predicted action: `next_token[0]`\n",
    "    obs, reward, terminated, truncated, info = env.step(get_action(next_token[0], env))\n",
    "    \n",
    "    # Prepare the next observation.\n",
    "    obs['direction'] = np.array([obs['direction']])\n",
    "    obs['image'] = np.array([obs['image']])\n",
    "    obs['mission'] = [obs['mission']]\n",
    "    # Prepare a temporary action token. Will be sliced off after sequencing.\n",
    "    # We just need this because each modality of the episodes need to have \n",
    "    # the same `E` dimension (remember - (B, E, T, C)), so that we can \n",
    "    # concatenate them on the `T` dimension.\n",
    "    dummy_action = 0\n",
    "    obs['action'] = np.array([dummy_action])\n",
    "    \n",
    "    # Move old tensors to CPU to free GPU memory\n",
    "    xs = xs.to(\"cpu\")\n",
    "    \n",
    "    xs_new = tokenize(obs)\n",
    "    # Merge the new episode.\n",
    "    xs = Timesteps([\n",
    "        (k, torch.concat([xs[k], xs_new[k].to(\"cpu\").unsqueeze(0)])) for k in xs.keys()\n",
    "    ])\n",
    "    \n",
    "    # Only move to GPU right before model inference\n",
    "    xs = xs.to(device)\n",
    "    \n",
    "    # Predict the next action\n",
    "    with torch.no_grad():  # Use mixed precision to reduce memory\n",
    "        logits, loss = trained_model(xs, pad=False, sequence=sequence_four_rooms)\n",
    "    \n",
    "    temp = 0.8\n",
    "    scaled_logits = logits / temp\n",
    "    probs = scaled_logits.softmax(dim=2)\n",
    "    next_token = torch.multinomial(probs[0, [-1]], num_samples=1)\n",
    "    next_token = tokenizer.decode_discrete(next_token)\n",
    "    \n",
    "    # Move tensors back to CPU and clear GPU cache\n",
    "    xs = xs.to(\"cpu\")\n",
    "    logits = logits.to(\"cpu\")\n",
    "    probs = probs.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fc18534",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af869a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f634b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
